import os, torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torchvision.models import resnet50
from torch.utils.data import DataLoader, Subset
from torch.optim.lr_scheduler import StepLR
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
from tqdm import tqdm
from PIL import Image

# â”€ ì‚¬ìš©ì ì •ì˜ ì´ë¯¸ì§€ ë¡œë” â”€

def pil_loader_with_rgb(path):
    img = Image.open(path)
    if img.mode in ('P', 'RGBA'):
        img = img.convert('RGB')
    return img

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ì„¤ì •â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

data_dir      = './model_2/train'  # ë°ì´í„° í¬ë²„ (fresh, moldy í•˜ìœ„ í´ë˜ìŠ¤ í¬í•¨)
num_classes   = 2                  # í´ë˜ìŠ¤ ìˆ˜ (ê³°íŒì´/ì‹ ì„ )
num_epochs    = 100                # ìµœëŒ€ í•™ìŠµ epoch
batch_size = 4                     # í•„ìˆ˜
torch.backends.cudnn.benchmark = True  # ì¶”ê°€ ê¶Œì¥
learning_rate = 5e-5              # ì´ˆê¸° í•™ìŠµë¥  (ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œ ë” ì‘ê²Œ)
patience      = 7                  # EarlyStopping: í˜‘ìƒ ì—†ëŠ” epoch ìˆ˜
min_delta     = 1e-4               # EarlyStopping: ìµœì†Œ í˜‘ìƒ í¬ê±´

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("CUDA ì‚¬ìš© ìœ ë¬´:", device)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ë°ì´í„° ì „ì²˜ë¦¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

train_transform = transforms.Compose([
    transforms.Lambda(lambda img: img.convert("RGB")),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(0.05, 0.05, 0.05, 0.01),
    transforms.RandomPerspective(p=0.1),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

val_transform = transforms.Compose([
    transforms.Lambda(lambda img: img.convert("RGB")),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ë°ì´í„°ì…‹ ë¡œë”© ë° ë¶„ë¦¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

full_dataset = datasets.ImageFolder(data_dir, transform=train_transform, loader=pil_loader_with_rgb)
val_dataset  = datasets.ImageFolder(data_dir, transform=val_transform, loader=pil_loader_with_rgb)

val_size     = int(0.2 * len(full_dataset))
train_size   = len(full_dataset) - val_size
indices = torch.randperm(len(full_dataset))
train_indices = indices[:train_size]
val_indices   = indices[train_size:]

train_dataset = Subset(full_dataset, train_indices)
val_dataset   = Subset(val_dataset, val_indices)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=4)
val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ëª¨ë¸ êµ¬ì„±: ResNet50 scratch í•™ìŠµâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

model = resnet50(weights=None)  # ResNet50 ì‚¬ìš©, ì‚¬ì „í•™ì… ì—†ì´ scratch í•™ìŠµ

# â”€ ê°€ìš´ì¹˜ ì´ˆê¸°í™” â”€
def initialize_weights(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
        nn.init.kaiming_normal_(m.weight)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)

model.apply(initialize_weights)

# â”€ FC ë ˆì´ì–´ ìˆ˜ì • (ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜ì— ë§ì¶°) â”€
model.fc = nn.Sequential(
    nn.Linear(model.fc.in_features, 256),
    nn.BatchNorm1d(256),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(128, num_classes)
)
model.to(device)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€í•™ìŠµ ì„¤ì •â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = StepLR(optimizer, step_size=10, gamma=0.5)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€í•™ìŠµ ë£¨í”„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

best_val_acc = 0.0
counter = 0
train_loss_list, val_loss_list, val_acc_list = [], [], []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for inputs, labels in tqdm(train_loader, desc=f"[Epoch {epoch+1}] Training"):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    train_loss = running_loss / len(train_loader)
    train_loss_list.append(train_loss)

    # â”€ ê²€ì§€ â”€
    model.eval()
    val_loss, correct, total = 0.0, 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            val_loss += criterion(outputs, labels).item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss /= len(val_loader)
    val_acc = 100 * correct / total
    val_loss_list.append(val_loss)
    val_acc_list.append(val_acc)

    print(f"â”” Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%")

    if val_acc > best_val_acc + 1e-4:
        best_val_acc = val_acc
        counter = 0
        torch.save(model.state_dict(), 'best_model_123.pth')
        print("  âœ” ëª¨ë¸ ì €ì¥ë¨ (val_acc ê°ì†Œ)")
    else:
        counter += 1
        print(f"  âœ˜ ê°ì†Œ ì—†ìŒ â†’ EarlyStopping ì¹´ìš´í„°: {counter}/{patience}")
        if counter >= patience:
            print("  âš  ê°ì†Œ ì—†ì–´ì„œ ë©ˆì¶©")
            break

    scheduler.step()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ê°€ì¥ ë†’ì€ val acc ì¶œë ¥â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nğŸ¯ ìµœê³  ê²€ì§€ ì •í™•ë„ (Best Validation Accuracy): {best_val_acc:.2f}%")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ì „ì²´ ë°ì´í„°ë¡œ ë¯¸ì„¸ì¡°ì •â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ (fine-tuning) ì‹œì‘")
full_dataset = datasets.ImageFolder(data_dir, transform=train_transform, loader=pil_loader_with_rgb)
full_loader  = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, num_workers=4)

model.load_state_dict(torch.load('best_model_123.pth'))
model.train()

for epoch in range(3):
    running_loss = 0.0
    for inputs, labels in tqdm(full_loader, desc=f"[FineTune {epoch+1}]"):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"  â¡ Fine-tuning Epoch {epoch+1}: Loss {running_loss / len(full_loader):.4f}")

torch.save(model.state_dict(), 'best_model_final_0703.pth')
print("\nì „ì²´ ë°ì´í„° ê¸°ë°˜ ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë° ì‹œê°í™”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=datasets.ImageFolder(data_dir).classes))

# â”€ ì†ìŠ¤ ë° ì •í™•ë„ ê·¸ë˜í”„ ì‹œê°í™” â”€
plt.plot(train_loss_list, label='Train Loss')
plt.plot(val_loss_list, label='Val Loss')
plt.legend(); plt.grid(); plt.title("Loss per Epoch")
plt.show()

plt.plot(val_acc_list, label='Val Accuracy')
plt.legend(); plt.grid(); plt.title("Validation Accuracy")
plt.show()
